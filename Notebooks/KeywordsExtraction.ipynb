{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction \n",
    "\n",
    "Based on: Social Media Analytics – Introduction to Text Mining – Keywords extraction (using RAKE method)\n",
    "\n",
    "by (c) Nuno Antonio 2019-2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from rake_nltk import Rake\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "base_path = \"Data/\"\n",
    "ds = pd.read_excel(base_path + \"Tweets_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace emojis and smileys\n",
    "\n",
    "# Converting emojis to words\n",
    "# Using both emot and emoji package to cover missing emojis\n",
    "def convert_emojis(text):\n",
    "    # from https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "\n",
    "    emoji.demojize(text, delimiters=(\"\", \"\")) \n",
    "    return text\n",
    "# Converting emoticons to words   \n",
    "# from https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"\")\n",
    "    return text\n",
    "\n",
    "def remove_emoticon(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def textPreProcess(rawText, removeHTML=True, charsToRemove = r'\\?|\\.|\\!|\\;|\\.|\\\"|\\,|\\(|\\)|\\&|\\:|\\-', removeNumbers=True, removeLineBreaks=False, specialCharsToRemove = r'[^\\x00-\\xfd]', convertToLower=True, removeConsecutiveSpaces=True, convert_emojis=False, remove_emojis = False):\n",
    "    cleanedText = []\n",
    "    for x in (rawText[:]): \n",
    "        if type(x) != str:\n",
    "            print(\"Type: \", str(type(x)))\n",
    "            x = str(x)\n",
    "\n",
    "        # Remove HTML\n",
    "        if removeHTML:\n",
    "            procText = BeautifulSoup(x,'html.parser').get_text()\n",
    "\n",
    "        if convert_emojis:\n",
    "            procText = convert_emojis(procText)\n",
    "            procText = convert_emoticons(procText)\n",
    "        \n",
    "        if remove_emojis:\n",
    "            procText = remove_emoji(procText)\n",
    "            procText = remove_emoticon(procText)\n",
    "\n",
    "         # Remove punctuation and other special characters\n",
    "        if len(charsToRemove)>0:\n",
    "            procText = re.sub(charsToRemove,' ',procText)\n",
    "\n",
    "        # Remove numbers\n",
    "        if removeNumbers:\n",
    "            procText = re.sub(r'\\d+',' ',procText)\n",
    "\n",
    "        # Remove line breaks\n",
    "        if removeLineBreaks:\n",
    "            procText = procText.replace('\\n',' ').replace('\\r', '')\n",
    "\n",
    "        # Remove special characters\n",
    "        if len(specialCharsToRemove)>0:\n",
    "            procText = re.sub(specialCharsToRemove,' ',procText)\n",
    "\n",
    "        # Normalize to lower case\n",
    "        if convertToLower:\n",
    "            procText = procText.lower() \n",
    "\n",
    "        # Replace multiple consecutive spaces with just one space\n",
    "        if removeConsecutiveSpaces:\n",
    "            procText = re.sub(' +', ' ', procText)\n",
    "\n",
    "        # If there is a text, add it to the clean text         \n",
    "        cleanedText.append(procText)\n",
    "\n",
    "\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "def tokenize_words(texts):\n",
    "    words_new = []\n",
    "    for w in (texts[:]):\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            words_new.append(w_token)\n",
    "    return words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize words\n",
    "def lemmatize(words):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  procText = []\n",
    "  for w in (words[:]):\n",
    "    lemmatized_word = [lemmatizer.lemmatize(x) for x in (w[:])]\n",
    "    procText.append(lemmatized_word)\n",
    "  return procText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def removeStopWords(texts, stop_words):\n",
    "  procText = []\n",
    "  for t in (texts[:]):\n",
    "    cleaned_text = [w for w in t[:] if not w in stop_words]\n",
    "    procText.append(cleaned_text)\n",
    "  return procText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recreate text from words\n",
    "def recreateText(words):\n",
    "    text_new = []\n",
    "    for w in (words[:]):\n",
    "        temp_str = (' ').join(w)\n",
    "        text_new.append(temp_str)\n",
    "    return text_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:  <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with only the description\n",
    "ppText = textPreProcess(ds.text, charsToRemove ='', removeNumbers=False, removeLineBreaks=True)\n",
    "processedTweets = pd.DataFrame(data=ppText, index=ds.index, columns=['PreProcessedText']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty text\n",
    "processedTweets.PreProcessedText = processedTweets.PreProcessedText.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAKE method - in English\n",
    "r = Rake(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords extraction per review\n",
    "r.extract_keywords_from_sentences(processedTweets['PreProcessedText'])\n",
    "phrases = r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!! paraguay needs vaccine !!!!',\n",
       " 'pastor thai jeremiah nguyen tom adsit trang adsit leann adsit tu trang nguyen excuses pastor phu',\n",
       " 'phu pham thai nguyen chinh nguyen tom trang leann adsit jeremiah nguyen',\n",
       " 'congressio har jagah badnam krne aa jata hi jese chacha wese',\n",
       " 'america ga beynun kuran huhdha nudheehuri astrazenaca vaccine ge 60 million doze sending',\n",
       " 'fatemi khalifatullah mowlana kareem shah al hussayni aga khan iv',\n",
       " 'profit ... india hai ... bade dil walo ka desh hai',\n",
       " 'souza interviews k sujatha rao == may 19th 2021 ==== =====',\n",
       " 'pappua tha jo vaccine ke liy fund de rha tha govt',\n",
       " 'dawai kare ya yaha par situation co trio mein lye',\n",
       " 'country muted version 500 million doller withoutloan trapped give economy restoreand uk 5',\n",
       " 'occupation saves lives banning arms deals saves lives recognizing rights saves lives respecting',\n",
       " 'law saves lives banning arms deals saves lives banning arms deals saves lives',\n",
       " 'consider hepa purifiers clean indoor air covid floats around like smoke indoors',\n",
       " 'aur tum kya kar rahe ho packistan mein',\n",
       " 'england .. oxford .. moderna .. 3 months later .. jj ... 5 months later ..',\n",
       " 'highly principled successful intellectual giant like von der leyen speaks',\n",
       " 'mas vacunas porfavor paraguay need vaccine',\n",
       " 'boost vaccination drive sir paracetamol ki jaghe beer please sir',\n",
       " 'gli usa comprano 500 milioni dosi da donare']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "phrases[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"Data/phrases.txt\", \"w\")\n",
    "for element in phrases:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_300 = phrases[0:300]\n",
    "textfile = open(\"Data/phrases300.txt\", \"w\")\n",
    "for element in phrases_300:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd0cbc9b7885ccecc363ff0025696a0c3f7da593d3e68eaace78b68fd615729d33b",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}