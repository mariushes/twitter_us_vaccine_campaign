{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Analysis – Lexical approach\n",
    "Based on Sentiment Analysis Notebook by (c) Nuno Antonio 2019-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "base_path = \"Data/\"\n",
    "ds = pd.read_excel(base_path + \"Tweets_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def textPreProcess(rawText, removeHTML=True, charsToRemove = r'\\?|\\.|\\!|\\;|\\.|\\\"|\\,|\\(|\\)|\\&|\\:|\\-', removeNumbers=True, removeLineBreaks=False, specialCharsToRemove = r'[^\\x00-\\xfd]', convertToLower=True, removeConsecutiveSpaces=True, convert_emojis=False, remove_emojis = False):\n",
    "    cleanedText = []\n",
    "    for x in (rawText[:]): \n",
    "        \n",
    "\n",
    "        if type(x) != str:\n",
    "            print(\"Type: \", str(type(x)))\n",
    "            x = str(x)\n",
    "\n",
    "        # Remove HTML\n",
    "        if removeHTML:\n",
    "            procText = BeautifulSoup(x,'html.parser').get_text()\n",
    "\n",
    "        if convert_emojis:\n",
    "            procText = convert_emojis(procText)\n",
    "            procText = convert_emoticons(procText)\n",
    "        \n",
    "        if remove_emojis:\n",
    "            procText = remove_emoji(procText)\n",
    "            procText = remove_emoticon(procText)\n",
    "\n",
    "         # Remove punctuation and other special characters\n",
    "        if len(charsToRemove)>0:\n",
    "            procText = re.sub(charsToRemove,' ',procText)\n",
    "\n",
    "        # Remove numbers\n",
    "        if removeNumbers:\n",
    "            procText = re.sub(r'\\d+',' ',procText)\n",
    "\n",
    "        # Remove line breaks\n",
    "        if removeLineBreaks:\n",
    "            procText = procText.replace('\\n',' ').replace('\\r', '')\n",
    "\n",
    "        # Remove special characters\n",
    "        if len(specialCharsToRemove)>0:\n",
    "            procText = re.sub(specialCharsToRemove,' ',procText)\n",
    "\n",
    "        # Normalize to lower case\n",
    "        if convertToLower:\n",
    "            procText = procText.lower() \n",
    "\n",
    "        # Replace multiple consecutive spaces with just one space\n",
    "        if removeConsecutiveSpaces:\n",
    "            procText = re.sub(' +', ' ', procText)\n",
    "\n",
    "        # If there is a text, add it to the clean text         \n",
    "        if procText != '':\n",
    "            cleanedText.append(procText)\n",
    "\n",
    "\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace emojis and smileys\n",
    "\n",
    "# Converting emojis to words\n",
    "# Using both emot and emoji package to cover missing emojis\n",
    "def convert_emojis(text):\n",
    "    # from https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "\n",
    "    emoji.demojize(text, delimiters=(\"\", \"\")) \n",
    "    return text\n",
    "# Converting emoticons to words   \n",
    "# from https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"\")\n",
    "    return text\n",
    "\n",
    "def remove_emoticon(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"\", text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "def tokenize_words(texts):\n",
    "    words_new = []\n",
    "    for w in (texts[:]):\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            words_new.append(w_token)\n",
    "    return words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recreate text from words\n",
    "def recreateText(words):\n",
    "    text_new = []\n",
    "    for w in (words[:]):\n",
    "        temp_str = (' ').join(w)\n",
    "        text_new.append(temp_str)\n",
    "    return text_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type:  <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with only the description\n",
    "# Do not remove additional special characters and not convert to lower as they can make a difference in sentiment\n",
    "ppText = textPreProcess(ds.text, removeLineBreaks=True, removeNumbers=True, convertToLower=False, convert_emojis=False, remove_emojis=True)\n",
    "text_tokens = tokenize_words(ppText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Hate speech data/expandedLexicon.txt\") as f:\n",
    "    contents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_lexicon = [content.strip().split(\"_\")[0] for content in contents]\n",
    "hate_speech_lexicon = hate_speech_lexicon[0:1237]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_lexicon.remove(\"horrible\")\n",
    "hate_speech_lexicon.remove(\"horrible\")\n",
    "hate_speech_lexicon.remove(\"disgusting\")\n",
    "hate_speech_lexicon.remove(\"revolting\")\n",
    "hate_speech_lexicon.remove(\"hideous\")\n",
    "hate_speech_lexicon.remove(\"psychopath\")\n",
    "hate_speech_lexicon.remove(\"sociopath\")\n",
    "hate_speech_lexicon.remove(\"despicable\")\n",
    "hate_speech_lexicon.remove(\"hateful\")\n",
    "hate_speech_lexicon.remove(\"prank\")\n",
    "hate_speech_lexicon.remove(\"sexist\")\n",
    "hate_speech_lexicon.remove(\"cruel\")\n",
    "hate_speech_lexicon.remove(\"lousy\")\n",
    "hate_speech_lexicon.remove(\"weird\")\n",
    "hate_speech_lexicon.remove(\"inhuman\")\n",
    "hate_speech_lexicon.remove(\"insulting\")\n",
    "hate_speech_lexicon.remove(\"arrogant\")\n",
    "hate_speech_lexicon.remove(\"dishonest\")\n",
    "hate_speech_lexicon.remove(\"insulting\")\n",
    "hate_speech_lexicon.remove(\"reprehensible\")\n",
    "hate_speech_lexicon.remove(\"selfish\")\n",
    "hate_speech_lexicon.remove(\"shameless\")\n",
    "hate_speech_lexicon.remove(\"ungrateful\")\n",
    "hate_speech_lexicon.remove(\"cheating\")\n",
    "hate_speech_lexicon.remove(\"unprofessional\")\n",
    "hate_speech_lexicon.remove(\"immoral\")\n",
    "hate_speech_lexicon.remove(\"disrespectful\")\n",
    "hate_speech_lexicon.remove(\"ingrate\")\n",
    "hate_speech_lexicon.remove(\"delinquent\")\n",
    "hate_speech_lexicon.remove(\"shredding\")\n",
    "hate_speech_lexicon.remove(\"unethical\")\n",
    "hate_speech_lexicon.remove(\"twisted\")\n",
    "hate_speech_lexicon.remove(\"immorality\")\n",
    "hate_speech_lexicon.remove(\"were\")\n",
    "hate_speech_lexicon.remove(\"utter\")\n",
    "hate_speech_lexicon.remove(\"plagiarism\")\n",
    "hate_speech_lexicon.remove(\"decadent\")\n",
    "hate_speech_lexicon.remove(\"trucker\")\n",
    "hate_speech_lexicon.remove(\"ruthless\")\n",
    "hate_speech_lexicon.remove(\"unspeakable\")\n",
    "hate_speech_lexicon.remove(\"horrendous\")\n",
    "hate_speech_lexicon.remove(\"hokey\")\n",
    "hate_speech_lexicon.remove(\"rudeness\")\n",
    "hate_speech_lexicon.remove(\"hurtful\")\n",
    "hate_speech_lexicon.remove(\"queer\")\n",
    "\n",
    "hate_speech_lexicon.remove(\"suicidal\")\n",
    "hate_speech_lexicon.remove(\"envy\")\n",
    "hate_speech_lexicon.remove(\"despise\")\n",
    "hate_speech_lexicon.remove(\"187\")\n",
    "hate_speech_lexicon.remove(\"scary\")\n",
    "hate_speech_lexicon.remove(\"selfishness\")\n",
    "hate_speech_lexicon.remove(\"sore\")\n",
    "hate_speech_lexicon.remove(\"neurotic\")\n",
    "hate_speech_lexicon.remove(\"irresponsible\")\n",
    "hate_speech_lexicon.remove(\"grotesque\")\n",
    "hate_speech_lexicon.remove(\"insensitive\")\n",
    "hate_speech_lexicon.remove(\"genderqueer\")\n",
    "hate_speech_lexicon.remove(\"fearsome\")\n",
    "hate_speech_lexicon.remove(\"disagreeable\")\n",
    "hate_speech_lexicon.remove(\"scissor\")\n",
    "hate_speech_lexicon.remove(\"scissor\")\n",
    "hate_speech_lexicon.remove(\"immature\")\n",
    "hate_speech_lexicon.remove(\"scary\")\n",
    "hate_speech_lexicon.remove(\"ignorant\")\n",
    "hate_speech_lexicon.remove(\"lazy\")\n",
    "hate_speech_lexicon.remove(\"arent\")\n",
    "hate_speech_lexicon.remove(\"awful\")\n",
    "hate_speech_lexicon.remove(\"useless\")\n",
    "hate_speech_lexicon.remove(\"irresponsible\")\n",
    "hate_speech_lexicon.remove(\"paranoid\")\n",
    "hate_speech_lexicon.remove(\"pointless\")\n",
    "hate_speech_lexicon.remove(\"ignorant\")\n",
    "hate_speech_lexicon.remove(\"envy\")\n",
    "hate_speech_lexicon.remove(\"sickening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hate_list = []\n",
    "for tweet in text_tokens:\n",
    "    flag = False\n",
    "    for word in hate_speech_lexicon:\n",
    "        if word in tweet:\n",
    "            flag = True\n",
    "    hate_list.append(flag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sentiment for all sentences\n",
    "ds['Hate'] = hate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text user screen name  \\\n",
       "0      Doesn't vaccine approval typically take 6 and ...       spadesgeek   \n",
       "1      You're right, too bad this vaccine doesn't hav...         selirodz   \n",
       "2                                 The vaccine is free…❤️  FlowerGirlBaker   \n",
       "3      .      the world needs  vaccine access now. Th...    DevizesGreens   \n",
       "4      But you can get it.\\nI remember getting my Mum...    canfixstoopid   \n",
       "...                                                  ...              ...   \n",
       "22529  The country need to check on people who take o...         LSungoun   \n",
       "22530  Why can you not go maskless with out vaccine? ...         bcgov115   \n",
       "22531  You say the immunocomprimised \"should discuss ...       dmdmdtweet   \n",
       "22532  Well, Kate, not everyone is vaccinated and the...  TuffCrusherPlus   \n",
       "22533  I disagree. Look at the NYC and Charlottesvill...      rehtaeh1628   \n",
       "\n",
       "       user followers                                  url  \\\n",
       "0                  19           https://mobile.twitter.com   \n",
       "1                  78  http://twitter.com/download/android   \n",
       "2                1486  http://twitter.com/#!/download/ipad   \n",
       "3                 150           https://mobile.twitter.com   \n",
       "4                 355           https://mobile.twitter.com   \n",
       "...               ...                                  ...   \n",
       "22529              90  http://twitter.com/download/android   \n",
       "22530              38   http://twitter.com/download/iphone   \n",
       "22531               1  http://twitter.com/download/android   \n",
       "22532              83   http://twitter.com/download/iphone   \n",
       "22533             728   http://twitter.com/download/iphone   \n",
       "\n",
       "                      created at  replies  retweets  likes  \\\n",
       "0     2021-06-10 19:24:26.999999        0         0      0   \n",
       "1     2021-06-10 19:24:26.000000        0         0      0   \n",
       "2     2021-06-10 19:23:57.000000        0         0      0   \n",
       "3     2021-06-10 19:23:49.000000        0         0      0   \n",
       "4     2021-06-10 19:23:42.000000        0         0      0   \n",
       "...                          ...      ...       ...    ...   \n",
       "22529 2021-05-16 04:11:06.000000        0         0      0   \n",
       "22530 2021-05-16 04:11:01.000000        0         0      0   \n",
       "22531 2021-05-16 04:08:22.999999        0         0      1   \n",
       "22532 2021-05-16 04:08:03.000000        0         0      0   \n",
       "22533 2021-05-16 04:03:26.000000        0         0      1   \n",
       "\n",
       "                                                 mention  \\\n",
       "0      MatthewDavidH,EricTopol,TheEconomist,US_FDA,la...   \n",
       "1                                      DharkArk,JoeBiden   \n",
       "2                    theredshift11,POTUS,studentsfordemo   \n",
       "3      BorisJohnson,JustinTrudeau,POTUS,EUCouncil,Reg...   \n",
       "4      SerendipityOr,Shockwave_Shaun,Ozymandiyaas,Joe...   \n",
       "...                                                  ...   \n",
       "22529                  POTUS,HillaryClinton,Jaemyung_Lee   \n",
       "22530                                              POTUS   \n",
       "22531                                             CDCgov   \n",
       "22532                     50treeK8,OregonGovBrown,CDCgov   \n",
       "22533  RadioFreeTom,Ned_Newhouse,jeffjarvis,ashishkjh...   \n",
       "\n",
       "                               hashtag   Hate  \n",
       "0                                  NaN  False  \n",
       "1                                  NaN  False  \n",
       "2                                  NaN  False  \n",
       "3      COVID19,G7,EndThePandemic,COVAX  False  \n",
       "4                                  NaN  False  \n",
       "...                                ...    ...  \n",
       "22529                              NaN  False  \n",
       "22530                              NaN  False  \n",
       "22531                              NaN  False  \n",
       "22532                              NaN  False  \n",
       "22533                              NaN  False  \n",
       "\n",
       "[22534 rows x 11 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>user screen name</th>\n      <th>user followers</th>\n      <th>url</th>\n      <th>created at</th>\n      <th>replies</th>\n      <th>retweets</th>\n      <th>likes</th>\n      <th>mention</th>\n      <th>hashtag</th>\n      <th>Hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Doesn't vaccine approval typically take 6 and ...</td>\n      <td>spadesgeek</td>\n      <td>19</td>\n      <td>https://mobile.twitter.com</td>\n      <td>2021-06-10 19:24:26.999999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>MatthewDavidH,EricTopol,TheEconomist,US_FDA,la...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>You're right, too bad this vaccine doesn't hav...</td>\n      <td>selirodz</td>\n      <td>78</td>\n      <td>http://twitter.com/download/android</td>\n      <td>2021-06-10 19:24:26.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>DharkArk,JoeBiden</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The vaccine is free…❤️</td>\n      <td>FlowerGirlBaker</td>\n      <td>1486</td>\n      <td>http://twitter.com/#!/download/ipad</td>\n      <td>2021-06-10 19:23:57.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>theredshift11,POTUS,studentsfordemo</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>.      the world needs  vaccine access now. Th...</td>\n      <td>DevizesGreens</td>\n      <td>150</td>\n      <td>https://mobile.twitter.com</td>\n      <td>2021-06-10 19:23:49.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>BorisJohnson,JustinTrudeau,POTUS,EUCouncil,Reg...</td>\n      <td>COVID19,G7,EndThePandemic,COVAX</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But you can get it.\\nI remember getting my Mum...</td>\n      <td>canfixstoopid</td>\n      <td>355</td>\n      <td>https://mobile.twitter.com</td>\n      <td>2021-06-10 19:23:42.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SerendipityOr,Shockwave_Shaun,Ozymandiyaas,Joe...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22529</th>\n      <td>The country need to check on people who take o...</td>\n      <td>LSungoun</td>\n      <td>90</td>\n      <td>http://twitter.com/download/android</td>\n      <td>2021-05-16 04:11:06.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>POTUS,HillaryClinton,Jaemyung_Lee</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22530</th>\n      <td>Why can you not go maskless with out vaccine? ...</td>\n      <td>bcgov115</td>\n      <td>38</td>\n      <td>http://twitter.com/download/iphone</td>\n      <td>2021-05-16 04:11:01.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>POTUS</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22531</th>\n      <td>You say the immunocomprimised \"should discuss ...</td>\n      <td>dmdmdtweet</td>\n      <td>1</td>\n      <td>http://twitter.com/download/android</td>\n      <td>2021-05-16 04:08:22.999999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>CDCgov</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22532</th>\n      <td>Well, Kate, not everyone is vaccinated and the...</td>\n      <td>TuffCrusherPlus</td>\n      <td>83</td>\n      <td>http://twitter.com/download/iphone</td>\n      <td>2021-05-16 04:08:03.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50treeK8,OregonGovBrown,CDCgov</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22533</th>\n      <td>I disagree. Look at the NYC and Charlottesvill...</td>\n      <td>rehtaeh1628</td>\n      <td>728</td>\n      <td>http://twitter.com/download/iphone</td>\n      <td>2021-05-16 04:03:26.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>RadioFreeTom,Ned_Newhouse,jeffjarvis,ashishkjh...</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>22534 rows × 11 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_excel(base_path + \"Tweets_Hate.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd0cbc9b7885ccecc363ff0025696a0c3f7da593d3e68eaace78b68fd615729d33b",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}